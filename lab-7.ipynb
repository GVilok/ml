{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use cross-validation techniques (RandomizedSearchCV()) technique to tune the hyperparameters for your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for SVM: {'kernel': 'linear', 'C': 0.1}\n",
      "Best parameters for Decision Tree: {'max_depth': 10}\n",
      "Best parameters for Random Forest: {'n_estimators': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for AdaBoost: {'n_estimators': 50}\n",
      "Best parameters for XGBoost: {'n_estimators': 50}\n",
      "Skipping hyperparameter tuning for Naive Bayes (no parameters to tune).\n",
      "Best parameters for MLP: {'hidden_layer_sizes': (100,), 'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load Data\n",
    "file_path = \"student_embeddings.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "\n",
    "# Clean Data\n",
    "df_cleaned = df.drop(columns=['Student'])  # Drop 'Student' column\n",
    "target_col = df_cleaned.columns[-1]  # Last column as target\n",
    "\n",
    "# Handle NaN values in target column\n",
    "df_cleaned[target_col] = pd.to_numeric(df_cleaned[target_col], errors='coerce')\n",
    "df_cleaned = df_cleaned.dropna(subset=[target_col])\n",
    "\n",
    "# Convert target column to integer\n",
    "df_cleaned[target_col] = df_cleaned[target_col].astype(int)\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "X = df_cleaned.drop(columns=[target_col])\n",
    "y = df_cleaned[target_col]\n",
    "\n",
    "# Normalize target variable (to start from 0)\n",
    "y -= y.min()\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define Models & Hyperparameters\n",
    "models = {\n",
    "    \"SVM\": (SVC(), {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {'max_depth': [3, 5, 10]}),\n",
    "    \"Random Forest\": (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "    \"AdaBoost\": (AdaBoostClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "    \"XGBoost\": (XGBClassifier(eval_metric='mlogloss'), {'n_estimators': [50, 100, 200]}),\n",
    "    \"Naive Bayes\": (GaussianNB(), {}),  # No hyperparameters to tune\n",
    "    \"MLP\": (MLPClassifier(max_iter=500), {'hidden_layer_sizes': [(50,), (100,)], 'alpha': [0.0001, 0.001]})\n",
    "}\n",
    "\n",
    "# Hyperparameter Tuning using RandomizedSearchCV\n",
    "best_models = {}\n",
    "for name, (model, param_grid) in models.items():\n",
    "    if not param_grid:\n",
    "        best_models[name] = model\n",
    "        print(f\"Skipping hyperparameter tuning for {name} (no parameters to tune).\")\n",
    "        continue\n",
    "\n",
    "    n_iter = min(5, len(list(param_grid.values())[0]))  # Limit search iterations\n",
    "\n",
    "    clf = RandomizedSearchCV(model, param_grid, cv=3, n_iter=n_iter, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_models[name] = clf.best_estimator_\n",
    "    print(f\"Best parameters for {name}: {clf.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For projects dealing with classification problem, employ various other classifiers such as Support Vector Machines, Decision Tree, RandomForest, CatBoost, AdaBoost, XGBoost, Na√Øve-Bayes & MLP. Tabulate your results for your problem using different performance metrics. Your tabulated results should compare between train and test results and make appropriate observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         3\n",
      "           2       1.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.83      0.33      0.22         6\n",
      "weighted avg       0.75      0.50      0.33         6\n",
      "\n",
      "\n",
      "Decision Tree Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50         1\n",
      "           1       1.00      0.67      0.80         3\n",
      "           2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.78      0.72      0.66         6\n",
      "weighted avg       0.89      0.67      0.71         6\n",
      "\n",
      "\n",
      "Random Forest Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         1\n",
      "           1       0.60      1.00      0.75         3\n",
      "           2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.87      0.50      0.47         6\n",
      "weighted avg       0.80      0.67      0.60         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AdaBoost Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.33      0.33      0.33         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.17         6\n",
      "   macro avg       0.11      0.11      0.11         6\n",
      "weighted avg       0.17      0.17      0.17         6\n",
      "\n",
      "\n",
      "XGBoost Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         1\n",
      "           1       0.60      1.00      0.75         3\n",
      "           2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.87      0.50      0.47         6\n",
      "weighted avg       0.80      0.67      0.60         6\n",
      "\n",
      "\n",
      "Naive Bayes Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         1\n",
      "           1       0.60      1.00      0.75         3\n",
      "           2       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.87      0.50      0.47         6\n",
      "weighted avg       0.80      0.67      0.60         6\n",
      "\n",
      "\n",
      "MLP Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         1\n",
      "           1       0.50      0.67      0.57         3\n",
      "           2       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.50         6\n",
      "   macro avg       0.67      0.39      0.36         6\n",
      "weighted avg       0.58      0.50      0.45         6\n",
      "\n",
      "\n",
      "Model Performance Comparison:\n",
      "           Model  Train Accuracy  Test Accuracy  Precision    Recall  F1 Score\n",
      "0            SVM             1.0       0.500000   0.750000  0.500000  0.333333\n",
      "1  Decision Tree             1.0       0.666667   0.888889  0.666667  0.705556\n",
      "2  Random Forest             1.0       0.666667   0.800000  0.666667  0.597222\n",
      "3       AdaBoost             1.0       0.166667   0.166667  0.166667  0.166667\n",
      "4        XGBoost             1.0       0.666667   0.800000  0.666667  0.597222\n",
      "5    Naive Bayes             1.0       0.666667   0.800000  0.666667  0.597222\n",
      "6            MLP             1.0       0.500000   0.583333  0.500000  0.452381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"student_embeddings.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "\n",
    "# Data Preprocessing\n",
    "df_cleaned = df.drop(columns=['Student'], errors='ignore')  # Drop non-numeric column if exists\n",
    "\n",
    "target_column = df_cleaned.columns[-1]  # Identify last column as target\n",
    "\n",
    "# Convert target column to numeric and drop invalid rows\n",
    "df_cleaned[target_column] = pd.to_numeric(df_cleaned[target_column], errors='coerce')\n",
    "df_cleaned = df_cleaned.dropna(subset=[target_column])\n",
    "\n",
    "# Define Features & Target\n",
    "X = df_cleaned.drop(columns=[target_column])\n",
    "y = df_cleaned[target_column].astype(int)\n",
    "\n",
    "# Normalize target variable\n",
    "y -= y.min()\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel='rbf', C=1),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', n_estimators=100),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), alpha=0.001, max_iter=500)\n",
    "}\n",
    "\n",
    "# Train Models & Evaluate\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    results.append([name, train_acc, test_acc, precision, recall, f1])\n",
    "    print(f\"\\n{name} Classification Report (Test Data):\")\n",
    "    print(classification_report(y_test, y_test_pred, zero_division=1))\n",
    "\n",
    "# Create Performance Summary DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Train Accuracy\", \"Test Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
